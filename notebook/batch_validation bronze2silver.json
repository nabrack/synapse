{
	"name": "batch_validation bronze2silver",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "newpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "12321e05-fe60-45ab-add5-0a864210ec5f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/78e47d63-c67e-4ea9-87b7-fd1f5613ae8f/resourceGroups/synapse-rg/providers/Microsoft.Synapse/workspaces/synapseparty/bigDataPools/newpool",
				"name": "newpool",
				"type": "Spark",
				"endpoint": "https://synapseparty.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/newpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run ArchAi"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run EnvironmentProperties"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# The folder name of the raw data to process. Scheduled jobs use the 'watermark to now' format, manual jobs may use any string.\r\n",
					"run_id = \"1970-01-01T00:00:00Z to 2025-03-03T17:38:06Z\"\r\n",
					"# When given a value, will update the watermark.txt of the data product with the provided string/datetime\r\n",
					"watermark = \"\""
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"source": [
					"data_source = 'pmPostgres'\r\n",
					"data_name = 'batch_validation'\r\n",
					"data_path = f'{Environment.Storage.value}/{data_source}/{data_name}'\r\n",
					"partition_by = []\r\n",
					"unique_by = {}\r\n",
					"unique_by['2022'] = {}\r\n",
					"unique_by['2023'] = {}\r\n",
					"unique_by['2024'] = {}\r\n",
					"unique_by['2025'] = {}\r\n",
					"unique_by['2026'] = {}\r\n",
					"unique_by['2022']['batch_validation_root'] = ['batch_validation_identifier', 'batch_validation_key']\r\n",
					"unique_by['2023']['batch_validation_root'] = ['batch_validation_identifier', 'batch_validation_key']\r\n",
					"unique_by['2024']['batch_validation_root'] = ['batch_validation_identifier', 'batch_validation_key']\r\n",
					"unique_by['2025']['batch_validation_root'] = ['batch_validation_identifier', 'batch_validation_key']\r\n",
					"unique_by['2026']['batch_validation_root'] = ['batch_validation_identifier', 'batch_validation_key']\r\n",
					"log = ArchAi.getLogger(mssparkutils.runtime.context['notebookname'])"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read in raw data, print inferred schema\r\n",
					"df = {}\r\n",
					"\r\n",
					"for plan_year in ['2022', '2023', '2024', '2025', '2026']:\r\n",
					"    df[plan_year] = spark.read.format('delta').option('basePath', f'{data_path}/bronze').load(f'{data_path}/bronze/plan_year={plan_year}').where(col('run_id') == run_id)\r\n",
					"    log.info(f'Loaded {df[plan_year].count()} rows of bronze {data_name} data from plan_year={plan_year}, run_id={run_id}')"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read in raw data, print inferred schema\r\n",
					"dfs_relationalized = {}\r\n",
					"\r\n",
					"for plan_year in ['2022', '2023', '2024', '2025', '2026']:\r\n",
					"    dfs_relationalized[plan_year] = ArchAi.relationalize(df[plan_year], data_name, unique_by[plan_year])\r\n",
					"    log.info(f'Split {data_name} into tables: {list(dfs_relationalized[plan_year].keys())}')\r\n",
					"for (name, table) in dfs_relationalized[plan_year].items():\r\n",
					"    # If the column is a string with the word datetime in it, convert it to UTC timestamp format\r\n",
					"         date_strings = [x[0] for x in dfs_relationalized[plan_year][name].dtypes if 'datetime' in x[0].lower() and x[1] == 'string']\r\n",
					"         print(date_strings)\r\n",
					"         for date_string in date_strings:\r\n",
					"             dfs_relationalized[plan_year][name] = dfs_relationalized[plan_year][name].withColumn(date_string, f.to_timestamp(col(date_string), 'yyyy-MM-dd HH:mm:ss.SSSSSSX'))\r\n",
					"for (name, table) in dfs_relationalized[plan_year].items():\r\n",
					"        print(name, f'count: {table.count()}')\r\n",
					"        table.printSchema()\r\n",
					"        "
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read in raw data, print inferred schema\r\n",
					"dfs_relationalized = {}\r\n",
					"\r\n",
					"for plan_year in ['2022', '2023', '2024', '2025', '2026']:\r\n",
					"    dfs_relationalized[plan_year] = ArchAi.relationalize(df[plan_year], data_name, unique_by[plan_year])\r\n",
					"    log.info(f'Split {data_name} into tables: {list(dfs_relationalized[plan_year].keys())}')\r\n",
					"    for (name, table) in dfs_relationalized[plan_year].items():\r\n",
					"        print(name, f'count: {table.count()}')\r\n",
					"        table.printSchema()"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"for plan_year in ['2022', '2023', '2024','2025','2026']:\r\n",
					"    for (name, table) in dfs_relationalized[plan_year].items():\r\n",
					"        warnings = ArchAi.validate_silver_df(table, unique_by[plan_year][name], name)\r\n",
					"        log.info(f\"New {name} data passes silver validations\")"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"for plan_year in ['2022', '2023', '2024','2025','2026']:\r\n",
					"    for (name, table) in dfs_relationalized[plan_year].items():\r\n",
					"        log.info(f'Upserting new data to {data_path}/silver/plan_year={plan_year}/{name}')\r\n",
					"        # Write dataframe out to a delta table, merging by ID\r\n",
					"        ArchAi.upsert_delta(table, f'{data_path}/silver/plan_year={plan_year}/{name}', unique_by[plan_year][name], partition_by)\r\n",
					"        warnings = ArchAi.validate_silver(f'{data_path}/silver/plan_year={plan_year}/{name}', unique_by[plan_year][name])\r\n",
					"        log.info(f\"Full {data_name} passed silver validations\")"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if watermark:\r\n",
					"    ArchAi.update_watermark(f'{data_path}/watermark.txt', watermark)"
				],
				"execution_count": 46
			}
		]
	}
}